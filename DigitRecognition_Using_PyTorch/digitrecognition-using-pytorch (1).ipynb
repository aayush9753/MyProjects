{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/digit-recognizer/train.csv\n/kaggle/input/digit-recognizer/test.csv\n/kaggle/input/digit-recognizer/sample_submission.csv\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Imprting the libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\nimport random\nimport math\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torchvision\nfrom torchvision import datasets, models, transforms\n\nimport torch.nn as nn\nfrom torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n\nfrom torch.utils.data import Dataset, DataLoader\n\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler, Adam, SGD\n\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\n# for evaluating the model\nfrom sklearn.metrics import accuracy_score\nfrom tqdm import tqdm\nimport time\nimport os\nimport copy","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing the Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train=pd.read_csv('/kaggle/input/digit-recognizer/train.csv')\ntest=pd.read_csv('/kaggle/input/digit-recognizer/test.csv')","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n0      1       0       0       0       0       0       0       0       0   \n1      0       0       0       0       0       0       0       0       0   \n2      1       0       0       0       0       0       0       0       0   \n3      4       0       0       0       0       0       0       0       0   \n4      0       0       0       0       0       0       0       0       0   \n\n   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n0       0  ...         0         0         0         0         0         0   \n1       0  ...         0         0         0         0         0         0   \n2       0  ...         0         0         0         0         0         0   \n3       0  ...         0         0         0         0         0         0   \n4       0  ...         0         0         0         0         0         0   \n\n   pixel780  pixel781  pixel782  pixel783  \n0         0         0         0         0  \n1         0         0         0         0  \n2         0         0         0         0  \n3         0         0         0         0  \n4         0         0         0         0  \n\n[5 rows x 785 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>pixel0</th>\n      <th>pixel1</th>\n      <th>pixel2</th>\n      <th>pixel3</th>\n      <th>pixel4</th>\n      <th>pixel5</th>\n      <th>pixel6</th>\n      <th>pixel7</th>\n      <th>pixel8</th>\n      <th>...</th>\n      <th>pixel774</th>\n      <th>pixel775</th>\n      <th>pixel776</th>\n      <th>pixel777</th>\n      <th>pixel778</th>\n      <th>pixel779</th>\n      <th>pixel780</th>\n      <th>pixel781</th>\n      <th>pixel782</th>\n      <th>pixel783</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 785 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing the Raw Data\nAs the Data is having 785 columns which have m 28*28*1 images and their labels.\nWe are converting the data in aproper format to feed them in the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Getting the label column\ntrain_labels = np.array(train['label'])\n# m = No of Exaples\nm_train = train.shape[0] #m in training data\nm_test = test.shape[0]  #m in testing data\n#reshaping the long 1D vector of shape 1*784 into a 3D vector of shape 1*28*28 \ntrain_data = np.array(train.loc[:,'pixel0':]).reshape(m_train,1,28,28)\ntest_data = np.array(test.loc[:,'pixel0':]).reshape(m_test,1,28,28)","execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualising the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"k = 0\nwhile k<2:\n    i = random.randint(0,42000)\n    plt.imshow(train_data[i,0,:,:])\n    plt.show()\n    print(f\"The label for the above image is {train_labels[i]}\")\n    k+=1","execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}},{"output_type":"stream","text":"The label for the above image is 1\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}},{"output_type":"stream","text":"The label for the above image is 8\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Splitting the data into Training, Validation and Testing Set\nWe are using 90% data for training, 5% for Validation and 5% for Testing purpose."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape, X_val.shape, X_test.shape","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"((33600, 1, 28, 28), (4200, 1, 28, 28), (4200, 1, 28, 28))"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Transformer\n Making a Transformer to convert our data into the PyTorch Tensors and Normalize the data.\n We are converting the data with Mean = 0.5 and STD = 0.5 across the channel"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numbers\nclass RandomRotation(object):\n\n\n    def __init__(self, degrees, resample=False, expand=False, center=None):\n        if isinstance(degrees, numbers.Number):\n            if degrees < 0:\n                raise ValueError(\"If degrees is a single number, it must be positive.\")\n            self.degrees = (-degrees, degrees)\n        else:\n            if len(degrees) != 2:\n                raise ValueError(\"If degrees is a sequence, it must be of len 2.\")\n            self.degrees = degrees\n\n        self.resample = resample\n        self.expand = expand\n        self.center = center\n\n    @staticmethod\n    def get_params(degrees):\n\n        angle = np.random.uniform(degrees[0], degrees[1])\n\n        return angle\n\n    def __call__(self, img):\n\n        \n        def rotate(img, angle, resample=False, expand=False, center=None):\n\n                \n            return img.rotate(angle, resample, expand, center)\n\n        angle = self.get_params(self.degrees)\n\n        return rotate(img, angle, self.resample, self.expand, self.center)","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class RandomShift(object):\n    def __init__(self, shift):\n        self.shift = shift\n        \n    @staticmethod\n    def get_params(shift):\n        hshift, vshift = np.random.uniform(-shift, shift, size=2)\n\n        return hshift, vshift \n    def __call__(self, img):\n        hshift, vshift = self.get_params(self.shift)\n        \n        return img.transform(img.size, Image.AFFINE, (1,0,hshift,0,1,vshift), resample=Image.BICUBIC, fill=1)","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# transformations to be applied on images\ntransform = transforms.Compose([transforms.ToTensor(),\n                              transforms.Normalize((0.5,), (0.5,)),\n                              ])","execution_count":11,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"class DigitDataset(Dataset):\n\n    def __init__(self,images,labels,transfrom = transform):\n        # Initialize data, download, etc.\n        self.x_data = torch.from_numpy(images/255.) # size [n_samples, n_features]\n        self.y_data = torch.from_numpy(labels) # size [n_samples, 1]\n        self.n_samples = images.shape[0]\n\n    # support indexing such that dataset[i] can be used to get i-th sample\n    def __getitem__(self, index):\n        return self.x_data[index], self.y_data[index]\n\n    # we can call len(dataset) to return the size\n    def __len__(self):\n        return self.n_samples","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = DigitDataset(X_train,y_train,transforms.Compose([RandomRotation(degrees=20), RandomShift(3),\n                                                                 transforms.Normalize(mean=(0.5,), std=(0.5,)),\n                             transform]))\nval_dataset = DigitDataset(X_val,y_val,transform)\ntest_dataset = DigitDataset(X_test,y_test,transform)","execution_count":13,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating Dataloaders\nWe will use Batch-Size = 64"},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size=64\n# defining trainloader, valloader and testloader\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=True)","execution_count":14,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Visualising the Data-loaders"},{"metadata":{"trusted":true},"cell_type":"code","source":"# shape of training data\ndataiter = iter(train_loader)\nimages, labels = dataiter.next()\n\nprint(images.shape)\nprint(labels.shape)\n\n# visualizing the training images\nplt.imshow(images[0].numpy().squeeze(), cmap='gray')","execution_count":15,"outputs":[{"output_type":"stream","text":"torch.Size([64, 1, 28, 28])\ntorch.Size([64])\n","name":"stdout"},{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"<matplotlib.image.AxesImage at 0x7f5a0df9a990>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# shape of validation data\ndataiter = iter(val_loader)\nimages, labels = dataiter.next()\n\nprint(images.shape)\nprint(labels.shape)\n\n# visualizing the training images\nplt.imshow(images[0].numpy().squeeze(), cmap='gray')","execution_count":16,"outputs":[{"output_type":"stream","text":"torch.Size([64, 1, 28, 28])\ntorch.Size([64])\n","name":"stdout"},{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"<matplotlib.image.AxesImage at 0x7f5a0db692d0>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAOk0lEQVR4nO3dYaxU9ZnH8d+z2hql1eAS8Cq4luqL1VWpIWgi0UqFgDFCTTAlcYNus5cXNQGzLxbFpJoNSdEt68ZE5DaQ4qZLU5UiwZpWCaLwAr0qXqEIqGHLLdd7l2BSIAgLPPviHswtzvnPZc6ZOSPP95PczMx55px5cvTHOTP/OfM3dxeAc9/fVN0AgNYg7EAQhB0IgrADQRB2IIjzW/liZsZH/0CTubvVWl7oyG5m081sl5l9bGYLi2wLQHNZo+PsZnaepN2SpkrqlfSOpDnu/sfEOhzZgSZrxpF9kqSP3f1Tdz8u6deSZhbYHoAmKhL2KyTtG/K4N1v2V8ys08y6zay7wGsBKKjIB3S1ThW+cpru7l2SuiRO44EqFTmy90oaN+TxWEn7i7UDoFmKhP0dSdeY2XfM7JuSfiRpXTltAShbw6fx7n7CzB6S9HtJ50la6e47SusMQKkaHnpr6MV4zw40XVO+VAPg64OwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIBqeshlfD9dee22yvnDhwmT99ttvT9ZPnTqVrC9fvjy31tvbm1y3p6cnWd+zZ0+yfvTo0WQ9mkJhN7O9kg5JOinphLtPLKMpAOUr48h+h7sfKGE7AJqI9+xAEEXD7pL+YGbvmllnrSeYWaeZdZtZd8HXAlBA0dP4W919v5mNlvSamX3k7m8OfYK7d0nqkiQz84KvB6BBhY7s7r4/ux2Q9FtJk8poCkD5Gg67mY0ws2+fvi9pmqTtZTUGoFzm3tiZtZmN1+DRXBp8O/Df7r64zjqcxjfB7Nmzc2uLFi1Krnvdddcl6x999FGh9VPj8GaWXLde/a233krWp0+fnls7l8fg3b3mjmv4Pbu7fyrpxoY7AtBSDL0BQRB2IAjCDgRB2IEgCDsQRMNDbw29GENvDXnggQeS9dRlpMeOHUuuO2PGjGT9wIH0NU533HFHsr558+bcWr3LZ5955plkvZ5p06bl1l5//fVC225neUNvHNmBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAh+SroNXHbZZcl6V1dXsp66XPOee+5Jrrtly5ZkvZ5du3Y1vO4FF1xQ6LXrqfdT1dFwZAeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIBhnbwOvvPJKsv7FF18k66nr3Tdt2tRISy0xatSoQusfPnw4Wd+3b1+h7Z9rOLIDQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCMs7fADTfckKzfeGN6Mty33347WV+7du1Z99Qq48ePz60tWLCg0LafeuqpZP3IkSOFtn+uqXtkN7OVZjZgZtuHLLvUzF4zsz3Z7cjmtgmgqOGcxv9S0pmz2i+UtMHdr5G0IXsMoI3VDbu7vynp4BmLZ0pald1fJWlWyX0BKFmj79nHuHufJLl7n5mNznuimXVK6mzwdQCUpOkf0Ll7l6QuiYkdgSo1OvTWb2YdkpTdDpTXEoBmaDTs6yTNze7PlfRyOe0AaJa687Ob2WpJ35c0SlK/pJ9KWivpN5KulPQnSbPd/cwP8Wpti9P4Gh555JFkffHixcn6+vXrc2svvvhiw+tK0sGD6f+sI0aMSNbXrFmTW5s6dWpy3dWrVyfrnZ3pj4KijrPnzc9e9z27u8/JKf2gUEcAWoqvywJBEHYgCMIOBEHYgSAIOxBE3aG3Ul+MobeabrnllmT9hRdeSNYvv/zy3JpZzVGYLx07dixZ37hxY7Le0dGRrKcu3122bFly3fnz5yfrJ06cSNajyht648gOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0Ewzv41UG8s++GHH86tTZkyJbnuTTfd1FBPw3X8+PHc2qRJk5Lr9vT0lN1OCIyzA8ERdiAIwg4EQdiBIAg7EARhB4Ig7EAQjLOf42bNSk/Dt3z58mT9oosuStY///zzZH3s2LG5tU8++SS57m233Zas9/X1JetRMc4OBEfYgSAIOxAEYQeCIOxAEIQdCIKwA0Ewzn4OSP02+xtvvJFc9+KLL07WZ86cmaxPmDAhWX/iiSdya/V+037JkiXJer2prqNqeJzdzFaa2YCZbR+y7HEz+7OZbcv+7iqzWQDlG85p/C8lTa+x/D/cfUL297ty2wJQtrphd/c3JR1sQS8AmqjIB3QPmVlPdpo/Mu9JZtZpZt1m1l3gtQAU1GjYl0n6rqQJkvok/Tzvie7e5e4T3X1ig68FoAQNhd3d+939pLufkvQLSemfCQVQuYbCbmZDf9v4h5K25z0XQHuoO85uZqslfV/SKEn9kn6aPZ4gySXtlTTP3eteXMw4e2PGjBmTrD/99NO5tbvvvju57owZM5L1zZs3J+v1LF26NLe2YMGC5LpHjx5N1q+++upkPer17nnj7OcPY8U5NRavKNwRgJbi67JAEIQdCIKwA0EQdiAIwg4EUffTeDRfvctMu7vT3zQ+dOhQbu36669Prrt3795kvagtW7bk1uoNvV144YXJ+rhx45L1qENveTiyA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQjLO3geeeey5Zv+SSS5L1+++/P7fW7HH0erZu3drwuidPnkzWjxw50vC2I+LIDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBMM7eApMnT07W77333mR948aNyfqmTZvOuqeyjB07NllfuXJlw9t+9tlnk/UdO3Y0vO2IOLIDQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBB1p2wu9cWCTtn86quvJuuTJk1K1q+88spkvcrruh988MFkfcWK/Al/d+7cmVx36tSpyfr+/fuT9ajypmyue2Q3s3FmttHMdprZDjObny2/1MxeM7M92e3IspsGUJ7hnMafkPQv7v73km6R9BMzu1bSQkkb3P0aSRuyxwDaVN2wu3ufu7+X3T8kaaekKyTNlLQqe9oqSbOa1SSA4s7qu/FmdpWk70naKmmMu/dJg/8gmNnonHU6JXUWaxNAUcMOu5l9S9JLkha4+1/Man4G8BXu3iWpK9tGyA/ogHYwrKE3M/uGBoP+K3dfky3uN7OOrN4haaA5LQIoQ90juw0ewldI2unuS4eU1kmaK+ln2e3LTenwa+Dmm29O1u+8885kvd7UwlUOrd13333JeldXV7Ke6v2xxx5LrsvQWrmGcxp/q6R/lPShmW3Llj2qwZD/xsx+LOlPkmY3p0UAZagbdnffLCnvDfoPym0HQLPwdVkgCMIOBEHYgSAIOxAEYQeC4KekSzByZPqCv/PPT+/mDz74oMx2zsrs2ekR0+effz5Z3717d7K+aNGi3NratWuT66JcHNmBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjG2UswenTNX+T60meffZas1/vJ5CeffDJZT10PP2/evOS6HR0dyXpvb2+yPmXKlGS9v78/WUfrcGQHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSCYsrkFikxrLEkHDhxI1t9///3c2sBAeu6OJUuWJOvbt29P1tF+Gp6yGcC5gbADQRB2IAjCDgRB2IEgCDsQBGEHgqg7zm5m4yQ9L+kySackdbn7f5rZ45L+WdL/Zk991N1/V2dbIcfZgVbKG2cfTtg7JHW4+3tm9m1J70qaJek+SYfd/d+H2wRhB5ovL+zDmZ+9T1Jfdv+Qme2UdEW57QFotrN6z25mV0n6nqSt2aKHzKzHzFaaWc05kMys08y6zay7UKcAChn2d+PN7FuSNkla7O5rzGyMpAOSXNK/afBU/5/qbIPTeKDJGn7PLklm9g1J6yX93t2X1qhfJWm9u/9Dne0QdqDJGr4QxsxM0gpJO4cGPfvg7rQfSuLyKKCNDefT+MmS3pL0oQaH3iTpUUlzJE3Q4Gn8Xknzsg/zUtviyA40WaHT+LIQdqD5uJ4dCI6wA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQRN0fnCzZAUn/M+TxqGxZO2rX3tq1L4neGlVmb3+XV2jp9exfeXGzbnefWFkDCe3aW7v2JdFbo1rVG6fxQBCEHQii6rB3Vfz6Ke3aW7v2JdFbo1rSW6Xv2QG0TtVHdgAtQtiBICoJu5lNN7NdZvaxmS2sooc8ZrbXzD40s21Vz0+XzaE3YGbbhyy71MxeM7M92W3NOfYq6u1xM/tztu+2mdldFfU2zsw2mtlOM9thZvOz5ZXuu0RfLdlvLX/PbmbnSdotaaqkXknvSJrj7n9saSM5zGyvpInuXvkXMMzsNkmHJT1/emotM3tS0kF3/1n2D+VId//XNuntcZ3lNN5N6i1vmvEHVOG+K3P680ZUcWSfJOljd//U3Y9L+rWkmRX00fbc/U1JB89YPFPSquz+Kg3+z9JyOb21BXfvc/f3svuHJJ2eZrzSfZfoqyWqCPsVkvYNedyr9prv3SX9wczeNbPOqpupYczpabay29EV93OmutN4t9IZ04y3zb5rZPrzoqoIe62padpp/O9Wd79J0gxJP8lOVzE8yyR9V4NzAPZJ+nmVzWTTjL8kaYG7/6XKXoaq0VdL9lsVYe+VNG7I47GS9lfQR03uvj+7HZD0Ww2+7Wgn/adn0M1uByru50vu3u/uJ939lKRfqMJ9l00z/pKkX7n7mmxx5fuuVl+t2m9VhP0dSdeY2XfM7JuSfiRpXQV9fIWZjcg+OJGZjZA0Te03FfU6SXOz+3MlvVxhL3+lXabxzptmXBXvu8qnP3f3lv9JukuDn8h/ImlRFT3k9DVe0gfZ346qe5O0WoOndf+nwTOiH0v6W0kbJO3Jbi9to97+S4NTe/doMFgdFfU2WYNvDXskbcv+7qp63yX6asl+4+uyQBB8gw4IgrADQRB2IAjCDgRB2IEgCDsQBGEHgvh/ftytuQQdwE4AAAAASUVORK5CYII=\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking the device type\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","execution_count":17,"outputs":[{"output_type":"execute_result","execution_count":17,"data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"I am using GPU to train the model hence everything is sent to GPU via *.cuda()* command"},{"metadata":{},"cell_type":"markdown","source":"# Model Building\nOur Model is taking 1 * 28 * 28 Images as input and having the output with dimension = 10"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Net(nn.Module):    \n    def __init__(self):\n        super(Net, self).__init__()\n          \n        self.features = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n          \n        self.classifier = nn.Sequential(\n            nn.Dropout(p = 0.5),\n            nn.Linear(64 * 7 * 7, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p = 0.5),\n            nn.Linear(512, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p = 0.5),\n            nn.Linear(512, 15),\n        )\n          \n\n                \n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        \n        return x   ","execution_count":18,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Defining the Optimizer, Criterion (loss function) and Learning Rate Scheduler."},{"metadata":{"trusted":true},"cell_type":"code","source":"# defining the model\nmodel = Net()\n# defining the optimizer\noptimizer = optim.Adam(model.parameters(), lr=0.003)\n# defining the loss function\ncriterion = nn.CrossEntropyLoss()\nstep_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n# checking if GPU is available\nif torch.cuda.is_available():\n    model = model.cuda()\n    criterion = criterion.cuda()\n    \nprint(model)","execution_count":19,"outputs":[{"output_type":"stream","text":"Net(\n  (features): Sequential(\n    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): ReLU(inplace=True)\n    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (7): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (9): ReLU(inplace=True)\n    (10): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (12): ReLU(inplace=True)\n    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (classifier): Sequential(\n    (0): Dropout(p=0.5, inplace=False)\n    (1): Linear(in_features=16384, out_features=512, bias=True)\n    (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): ReLU(inplace=True)\n    (4): Dropout(p=0.5, inplace=False)\n    (5): Linear(in_features=512, out_features=512, bias=True)\n    (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (7): ReLU(inplace=True)\n    (8): Dropout(p=0.5, inplace=False)\n    (9): Linear(in_features=512, out_features=15, bias=True)\n  )\n)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Training of the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_sizes = {}\ndataset_sizes['train'] = len(train_dataset)\ndataset_sizes['val'] = len(val_dataset)","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(model, criterion, optimizer, scheduler, num_epochs=50):\n    since = time.time() #Return the time in seconds since the epoch as a floating point number\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    dataloaders = {}\n    dataloaders['train'] = train_loader\n    dataloaders['val'] = val_loader\n    \n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            # Iterate over data.\n            for inputs, labels in dataloaders[phase]:\n                inputs = inputs.to(device)\n                inputs = inputs.type(torch.cuda.FloatTensor)\n                labels = labels.to(device)\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        optimizer.zero_grad()\n                        loss.backward()\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n\n            if phase == 'train':\n                scheduler.step()\n\n            epoch_loss = running_loss / dataset_sizes[phase]\n            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n                phase, epoch_loss, epoch_acc))\n\n            # deep copy the model\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n\n        print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed // 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodel = model.to(device)\n\nmodel = train_model(model, criterion, optimizer, step_lr_scheduler, num_epochs=50)","execution_count":22,"outputs":[{"output_type":"stream","text":"Epoch 0/49\n----------\n","name":"stdout"},{"output_type":"error","ename":"RuntimeError","evalue":"mat1 dim 1 must match mat2 dim 0","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-f1dc0f10646f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_lr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-21-ba38498cee69>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;31m# track history if only in train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m                     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-18-ef3b5cd7e691>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1672\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1673\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1674\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1675\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1676\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: mat1 dim 1 must match mat2 dim 0"]}]},{"metadata":{},"cell_type":"markdown","source":"### Testing our model on test-data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# getting predictions on test set and measuring the performance\ncorrect_count, all_count = 0, 0\nfor images,labels in test_loader:\n  for i in range(len(labels)):\n    images = images.cuda()\n    images = images.type(torch.cuda.FloatTensor)\n    labels = labels.cuda()\n    img = images[i].view(1, 1, 28, 28)\n    with torch.no_grad():\n        logps = model(img)\n\n    \n    ps = torch.exp(logps)\n    probab = list(ps.cpu()[0])\n    pred_label = probab.index(max(probab))\n    true_label = labels.cpu()[i]\n    if(true_label == pred_label):\n      correct_count += 1\n    all_count += 1\n\nprint(\"Number Of Images Tested =\", all_count)\nprint(\"\\nModel Accuracy =\", (correct_count/all_count))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Predicting for unlabelled Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"test = test_data/255 #Normalizing the data\ntest = torch.from_numpy(test)  # Converting into Tensors\ntest = test.type(torch.cuda.FloatTensor) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with torch.no_grad():\n  outputs = model(test.cuda())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Outputs are the output of the final linear Layer having shape = 10"},{"metadata":{"trusted":true},"cell_type":"code","source":"ps = torch.exp(outputs)\n\n#max_value is the value of highest no. in each 10-dim vector \n#index is the index of that max value \nmax_value, index = torch.max(ps,axis=1) \n\nindex = index.cpu()\n#Converting Prediction to numpy for Submission\nprediction = index.numpy()\n\nprint(prediction.shape)\nprint(prediction[:5])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Saving the Prediction in the acceptable format "},{"metadata":{"trusted":true},"cell_type":"code","source":"k = np.arange(1,28001)\nsubmission = pd.DataFrame({\n        \"ImageId\":k ,\n        \"Label\": prediction\n\n    })\n\nsubmission.to_csv('Digit_Recognition_submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}