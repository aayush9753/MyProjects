{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/digit-recognizer/train.csv\n/kaggle/input/digit-recognizer/test.csv\n/kaggle/input/digit-recognizer/sample_submission.csv\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Imprting the libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\nimport random\nimport math\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torchvision\nfrom torchvision import datasets, models, transforms\n\nimport torch.nn as nn\nfrom torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n\nfrom torch.utils.data import Dataset, DataLoader\n\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler, Adam, SGD\n\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\n# for evaluating the model\nfrom sklearn.metrics import accuracy_score\nfrom tqdm import tqdm\nimport time\nimport os\nimport copy","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing the Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train=pd.read_csv('/kaggle/input/digit-recognizer/train.csv')\ntest=pd.read_csv('/kaggle/input/digit-recognizer/test.csv')","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n0      1       0       0       0       0       0       0       0       0   \n1      0       0       0       0       0       0       0       0       0   \n2      1       0       0       0       0       0       0       0       0   \n3      4       0       0       0       0       0       0       0       0   \n4      0       0       0       0       0       0       0       0       0   \n\n   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n0       0  ...         0         0         0         0         0         0   \n1       0  ...         0         0         0         0         0         0   \n2       0  ...         0         0         0         0         0         0   \n3       0  ...         0         0         0         0         0         0   \n4       0  ...         0         0         0         0         0         0   \n\n   pixel780  pixel781  pixel782  pixel783  \n0         0         0         0         0  \n1         0         0         0         0  \n2         0         0         0         0  \n3         0         0         0         0  \n4         0         0         0         0  \n\n[5 rows x 785 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>pixel0</th>\n      <th>pixel1</th>\n      <th>pixel2</th>\n      <th>pixel3</th>\n      <th>pixel4</th>\n      <th>pixel5</th>\n      <th>pixel6</th>\n      <th>pixel7</th>\n      <th>pixel8</th>\n      <th>...</th>\n      <th>pixel774</th>\n      <th>pixel775</th>\n      <th>pixel776</th>\n      <th>pixel777</th>\n      <th>pixel778</th>\n      <th>pixel779</th>\n      <th>pixel780</th>\n      <th>pixel781</th>\n      <th>pixel782</th>\n      <th>pixel783</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 785 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing the Raw Data\nAs the Data is having 785 columns which have m 28*28*1 images and their labels.\nWe are converting the data in aproper format to feed them in the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Getting the label column\ntrain_labels = np.array(train['label'])\n# m = No of Exaples\nm_train = train.shape[0] #m in training data\nm_test = test.shape[0]  #m in testing data\n#reshaping the long 1D vector of shape 1*784 into a 3D vector of shape 1*28*28 \ntrain_data = np.array(train.loc[:,'pixel0':]).reshape(m_train,1,28,28)\ntest_data = np.array(test.loc[:,'pixel0':]).reshape(m_test,1,28,28)","execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualising the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"k = 0\nwhile k<2:\n    i = random.randint(0,42000)\n    plt.imshow(train_data[i,0,:,:])\n    plt.show()\n    print(f\"The label for the above image is {train_labels[i]}\")\n    k+=1","execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAANwUlEQVR4nO3df6zV9X3H8dcLuECLmsAcSJCpNTSULBHsHazabTasrTVZ0WzSkqyh03ldollNuqXG/dCZdSHLbKu2a3etTLo4rElr5A+zSomZ1W7EK2EIBad1TBEKWpqCXcQLvPfH/bJd8JzvuZzv9/yA9/OR3Jxzvu/zPZ93Tnjx/Z7zOed8HBECcPab1OsGAHQHYQeSIOxAEoQdSIKwA0lM6eZgUz0tpmtGN4cEUnlbv9A7ccSNapXCbvtqSfdKmizpmxGxpuz+0zVDy7y8ypAASmyOTU1rbZ/G254s6WuSPiFpkaRVthe1+3gAOqvKa/alkl6OiFci4h1Jj0haUU9bAOpWJezzJL027vaeYttJbA/ZHrE9MqojFYYDUEWVsDd6E+Bdn72NiOGIGIyIwQFNqzAcgCqqhH2PpPnjbl8oaW+1dgB0SpWwPydpge1LbE+V9GlJG+ppC0Dd2p56i4ijtm+V9D2NTb2tjYgdtXUGoFaV5tkj4glJT9TUC4AO4uOyQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQqLdlse7ekw5KOSToaEYN1NAWgfpXCXvhIRLxZw+MA6CBO44EkqoY9JD1p+3nbQ43uYHvI9ojtkVEdqTgcgHZVPY2/MiL22p4taaPtXRHx9Pg7RMSwpGFJOs+zouJ4ANpU6cgeEXuLywOSHpO0tI6mANSv7bDbnmH73BPXJX1M0va6GgNQryqn8XMkPWb7xOP8c0T8Sy1d4bR42rSmtZ+tvLx03/Of2Vta3/XHc0vrP/7UN0rrz759vGnt95/8o9J9F37jrdL68a0/Kq3jZG2HPSJekXRZjb0A6CCm3oAkCDuQBGEHkiDsQBKEHUjCEd37UNt5nhXLvLxr450tyqbWJOnFrzSfFNn1ya/V3c5JJrU4XhxX86m3Vr59uHza75HfWFJaP/bGG22PfabaHJt0KA66UY0jO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kUccPTqLDXv3TD5bWd33y3i510l2fOndfaX3N0MrS+vwv5ptnL8ORHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSYJ69D/zktitK62tvuL9LnZxZHrjxq6X1u79Y/jPa2XBkB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkmGfvA1N/+83S+gfLfza+wi+zt/aB799cXr+zvPeXb5rXtLbts/e11dMJg9OOldb33NH88wsX/s0PK419Jmp5ZLe91vYB29vHbZtle6Ptl4rLmZ1tE0BVEzmNf0jS1adsu13SpohYIGlTcRtAH2sZ9oh4WtLBUzavkLSuuL5O0rU19wWgZu2+QTcnIvZJUnE5u9kdbQ/ZHrE9MqojbQ4HoKqOvxsfEcMRMRgRgwNq8U4TgI5pN+z7bc+VpOLyQH0tAeiEdsO+QdLq4vpqSY/X0w6ATmk5z257vaSrJJ1ve4+kOyWtkfSo7RslvSrp+k42eba7Z9GjpfUBTy6tj0b7Yy986g9L6wtWbymtH5tS/k9o5pKpTWut1nav6uh7KzwxZ6GWYY+IVU1Ky2vuBUAH8XFZIAnCDiRB2IEkCDuQBGEHkuArrn3geJT/nzsao+X7l3zJ9TP/dep3mE628E/2lo/9octK6z//i1+U1v/1svVNa538aq4kTX/THR7hzMKRHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSYJ69D9zwgz8orb/40eG2H/uSGT8tf+xvzymt/+P7/r60PnPS9NJ6J+fSF264pbT+/q/k+7noMhzZgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJ5tn7wMCe5j+3XNXds58rv0PThbtO6FxvVS1as6+0frRLfZwpOLIDSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiiZdhtr7V9wPb2cdvusv267a3F3zWdbRNAVRM5sj8kqdGyIl+OiMXF3xP1tgWgbi3DHhFPSzrYhV4AdFCV1+y32t5WnObPbHYn20O2R2yPjOpIheEAVNFu2L8u6VJJiyXtk3RPsztGxHBEDEbE4ICmtTkcgKraCntE7I+IYxFxXNIDkpbW2xaAurUVdttzx928TtL2ZvcF0B9afp/d9npJV0k63/YeSXdKusr2YkkhabekmzvY41nv4j//t9L6wveW/z76+uvub1pbMrWzH6UY8OTS+mi0/9hDr11VWo+fH2r/wRNqGfaIWNVg84Md6AVAB/EJOiAJwg4kQdiBJAg7kARhB5JwRIW5kdN0nmfFMi/v2nhZTLmg+bLLr19/aem+H7+hfFnjVj9FPanF8eJ4hUWbf2flTaV1P7u17cc+W22OTToUB92oxpEdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5JgyeazwNGf7G9am3N/85okfc9XlNbv/kKLJZ8r+MsDv1Zan7Lr1dL6sTqbSYAjO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kwTx7cu9f+WLPxn7ygfI5/tk/Lf+uPU4PR3YgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIJ59rPc7r/+UGn93y+6p8UjDFQa/76fLWxau2DtltJ92//FeTTS8shue77tp2zvtL3D9ueK7bNsb7T9UnE5s/PtAmjXRE7jj0r6fER8QNKvS7rF9iJJt0vaFBELJG0qbgPoUy3DHhH7ImJLcf2wpJ2S5klaIWldcbd1kq7tVJMAqjutN+hsXyxpiaTNkuZExD5p7D8ESbOb7DNke8T2yKiOVOsWQNsmHHbb50j6jqTbIuLQRPeLiOGIGIyIwQFNa6dHADWYUNhtD2gs6A9HxHeLzfttzy3qcyUd6EyLAOrQcurNtiU9KGlnRHxpXGmDpNWS1hSXj3ekQ7Tkac3PmJYt31G67zmTqp1tDXhyaf3hf/h409qct/kKazdNZJ79SkmfkfSC7RMLYt+hsZA/avtGSa9Kur4zLQKoQ8uwR8Qzkhou7i5peb3tAOgUPi4LJEHYgSQIO5AEYQeSIOxAEnzF9Szwyl9d3rT2+K/cV7pv1a+RPnRobml97lMHOzY2Tg9HdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1Ignn2M8CUC+aU1r/6e9/sUifv9ujvfqS0fnzHri51glY4sgNJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEsyznwneM720/Fvv+Z+ODX3ZszeU1i/a8ULHxka9OLIDSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBITWZ99vqRvSbpAYz/1PRwR99q+S9JNkt4o7npHRDzRqUYzi0OHS+uLf9h8LnzrFWsrjT3j++dU2h/9YyIfqjkq6fMRscX2uZKet72xqH05Iv6uc+0BqMtE1mffJ2lfcf2w7Z2S5nW6MQD1Oq3X7LYvlrRE0uZi0622t9lea3tmk32GbI/YHhnVkUrNAmjfhMNu+xxJ35F0W0QckvR1SZdKWqyxI/89jfaLiOGIGIyIwQFNq6FlAO2YUNhtD2gs6A9HxHclKSL2R8SxiDgu6QFJSzvXJoCqWobdtiU9KGlnRHxp3Pbxy3deJ2l7/e0BqIsjovwO9ocl/UDSC/r/VXbvkLRKY6fwIWm3pJuLN/OaOs+zYpmXV2wZQDObY5MOxUE3qk3k3fhnJDXamTl14AzCJ+iAJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJtPw+e62D2W9I+u9xm86X9GbXGjg9/dpbv/Yl0Vu76uztooj45UaFrob9XYPbIxEx2LMGSvRrb/3al0Rv7epWb5zGA0kQdiCJXod9uMfjl+nX3vq1L4ne2tWV3nr6mh1A9/T6yA6gSwg7kERPwm77atsv2n7Z9u296KEZ27ttv2B7q+2RHvey1vYB29vHbZtle6Ptl4rLhmvs9ai3u2y/Xjx3W21f06Pe5tt+yvZO2ztsf67Y3tPnrqSvrjxvXX/NbnuypP+U9FFJeyQ9J2lVRPyoq400YXu3pMGI6PkHMGz/pqS3JH0rIn612Pa3kg5GxJriP8qZEfGFPuntLklv9XoZ72K1ornjlxmXdK2kz6qHz11JXyvVheetF0f2pZJejohXIuIdSY9IWtGDPvpeRDwt6eApm1dIWldcX6exfyxd16S3vhAR+yJiS3H9sKQTy4z39Lkr6asrehH2eZJeG3d7j/prvfeQ9KTt520P9bqZBuacWGaruJzd435O1XIZ7246ZZnxvnnu2ln+vKpehL3RUlL9NP93ZURcLukTkm4pTlcxMRNaxrtbGiwz3hfaXf68ql6EfY+k+eNuXyhpbw/6aCgi9haXByQ9pv5binr/iRV0i8sDPe7n//TTMt6NlhlXHzx3vVz+vBdhf07SAtuX2J4q6dOSNvSgj3exPaN440S2Z0j6mPpvKeoNklYX11dLeryHvZykX5bxbrbMuHr83PV8+fOI6PqfpGs09o78jyX9WS96aNLX+yT9R/G3o9e9SVqvsdO6UY2dEd0o6ZckbZL0UnE5q496+yeNLe29TWPBmtuj3j6ssZeG2yRtLf6u6fVzV9JXV543Pi4LJMEn6IAkCDuQBGEHkiDsQBKEHUiCsANJEHYgif8FKDEMLzNrYNUAAAAASUVORK5CYII=\n"},"metadata":{"needs_background":"light"}},{"output_type":"stream","text":"The label for the above image is 9\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}},{"output_type":"stream","text":"The label for the above image is 9\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Splitting the data into Training, Validation and Testing Set\nWe are using 90% data for training, 5% for Validation and 5% for Testing purpose."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape, X_val.shape, X_test.shape","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"((33600, 1, 28, 28), (4200, 1, 28, 28), (4200, 1, 28, 28))"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Transformer\n Making a Transformer to convert our data into the PyTorch Tensors and Normalize the data.\n We are converting the data with Mean = 0.5 and STD = 0.5 across the channel"},{"metadata":{"trusted":true},"cell_type":"code","source":"# transformations to be applied on images\ntransform = transforms.Compose([transforms.ToTensor(),\n                              transforms.Normalize((0.5,), (0.5,)),\n                              ])","execution_count":9,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"class DigitDataset(Dataset):\n\n    def __init__(self,images,labels,transfrom = transform):\n        # Initialize data, download, etc.\n        self.x_data = torch.from_numpy(images/255.) # size [n_samples, n_features]\n        self.y_data = torch.from_numpy(labels) # size [n_samples, 1]\n        self.n_samples = images.shape[0]\n\n    # support indexing such that dataset[i] can be used to get i-th sample\n    def __getitem__(self, index):\n        return self.x_data[index], self.y_data[index]\n\n    # we can call len(dataset) to return the size\n    def __len__(self):\n        return self.n_samples","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = DigitDataset(X_train,y_train)\nval_dataset = DigitDataset(X_val,y_val)\ntest_dataset = DigitDataset(X_test,y_test)","execution_count":11,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating Dataloaders\nWe will use Batch-Size = 64"},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size=64\n# defining trainloader, valloader and testloader\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=True)","execution_count":12,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Visualising the Data-loaders"},{"metadata":{"trusted":true},"cell_type":"code","source":"# shape of training data\ndataiter = iter(train_loader)\nimages, labels = dataiter.next()\n\nprint(images.shape)\nprint(labels.shape)\n\n# visualizing the training images\nplt.imshow(images[0].numpy().squeeze(), cmap='gray')","execution_count":13,"outputs":[{"output_type":"stream","text":"torch.Size([64, 1, 28, 28])\ntorch.Size([64])\n","name":"stdout"},{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"<matplotlib.image.AxesImage at 0x7f57371e41d0>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# shape of validation data\ndataiter = iter(val_loader)\nimages, labels = dataiter.next()\n\nprint(images.shape)\nprint(labels.shape)\n\n# visualizing the training images\nplt.imshow(images[0].numpy().squeeze(), cmap='gray')","execution_count":14,"outputs":[{"output_type":"stream","text":"torch.Size([64, 1, 28, 28])\ntorch.Size([64])\n","name":"stdout"},{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"<matplotlib.image.AxesImage at 0x7f57371ffdd0>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAANw0lEQVR4nO3dXahddXrH8d+vmRkvMuNLHCPBCcb62lpsRoKIDsUyJmpuNEirudBIlTPiKBMtaLAXFYsYWke9cuCMI6Z16lDfGB1CZzRIU0MRY9CYxLc0xJrJMYfU4BhFrPHpxVmREz3rv477be2c5/uBw957PXvt9bDJL2vt/d9r/R0RAjDz/VHbDQAYDMIOJEHYgSQIO5AEYQeS+MYgN2abr/6BPosIT7W8qz277Yttv2l7u+1V3bwWgP5yp+PstmdJekvSYkm7JL0kaXlEbCusw54d6LN+7NnPkbQ9InZExKeSfiXp0i5eD0AfdRP2EyS9O+nxrmrZIWyP2N5oe2MX2wLQpW6+oJvqUOErh+kRMSppVOIwHmhTN3v2XZLmT3r8PUm7u2sHQL90E/aXJJ1q+yTb35J0paSne9MWgF7r+DA+Ij6zfaOk30qaJemhiNjas84A9FTHQ28dbYzP7EDf9eVHNQAOH4QdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiQx0EtJoz9mzZpVW7v55puL695+++3F+tFHH12s21OeYPWF1atX19buuuuu4rr79+8v1vH1sGcHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSS4uuwMcPfdd9fWbr311uK6b7zxRrG+Y8eOYn3p0qXFesm2bbVzgEqSVqxYUaxv2rSp423PZFxdFkiOsANJEHYgCcIOJEHYgSQIO5AEYQeSYJz9MDB79uxivTRWvmfPnuK6TePke/fuLdYXLlxYrD/22GO1tQULFhTXXbt2bbF+5ZVXFusfffRRsT5T1Y2zd3XxCts7JX0o6YCkzyJiUTevB6B/enGlmr+MiPJ//wBax2d2IIluwx6Sfmf7ZdsjUz3B9ojtjbY3drktAF3o9jD+/IjYbXuupGdtvxER6yc/ISJGJY1KfEEHtKmrPXtE7K5uxyU9JemcXjQFoPc6Drvt2ba/c/C+pCWStvSqMQC91c1h/PGSnqquG/4NSf8aEf/ek65wiGuuuaZYP+6442prTWPR4+PjnbT0haZzys8666za2oYNG4rrNv0GoHRNekm66aabivVsOg57ROyQ9Oc97AVAHzH0BiRB2IEkCDuQBGEHkiDsQBJM2TwEmk5hbaqffvrptbV33nmno556pXSa6XXXXVdcd/369cX6KaecUqyXprI+cOBAcd2ZiD07kARhB5Ig7EAShB1IgrADSRB2IAnCDiTBpaQxtPbt21esH3nkkcX6eeedV1t78cUXO+rpcMCUzUByhB1IgrADSRB2IAnCDiRB2IEkCDuQBOezY2g98sgjxfoNN9xQrJcuRT2Tx9nrsGcHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQYZ8fQ2r9/f9stzCiNe3bbD9ket71l0rI5tp+1/XZ1e0x/2wTQrekcxj8s6eIvLVslaV1EnCppXfUYwBBrDHtErJf0/pcWXyppTXV/jaTLetwXgB7r9DP78RExJkkRMWZ7bt0TbY9IGulwOwB6pO9f0EXEqKRRiQtOAm3qdOhtj+15klTdjveuJQD90GnYn5a0orq/QtKve9MOgH6ZztDbo5L+S9LptnfZvlbSakmLbb8taXH1GMAQa/zMHhHLa0o/7HEvAPqIn8sCSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEl5LG0Lr88svbbmFGYc8OJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kwzj4E5s6tnT1LknT22WcX65dccknH2z7zzDOL9a1bt3b82pK0YcOG2tonn3xSXHf+/PnFetOUzg888ECxng17diAJwg4kQdiBJAg7kARhB5Ig7EAShB1IwhExuI3Zg9vYELnggguK9YcffrhYbxpvzmrTpk3F+r333ltbe+GFF4rrvvvuux31NAwiwlMtn8787A/ZHre9ZdKyO2z/3vYr1d/SXjYLoPemcxj/sKSLp1h+X0QsrP7W9rYtAL3WGPaIWC/p/QH0AqCPuvmC7kbbm6vD/GPqnmR7xPZG2xu72BaALnUa9p9JOlnSQkljkn5a98SIGI2IRRGxqMNtAeiBjsIeEXsi4kBEfC7p55LO6W1bAHqto7Dbnjfp4TJJW+qeC2A4NI6z235U0gWSvitpj6S/rx4vlBSSdkr6UUSMNW5sho6zL1++vFgfHR0t1o844ohi/cEHHyzW166tHwxpOl+932677bba2lFHHTXATg718ccfF+tN59pfdNFFxXrTbwD6qW6cvfHiFREx1b/kX3TdEYCB4ueyQBKEHUiCsANJEHYgCcIOJMEprj3w+OOPF+vLli0r1nfv3l2sH86nuG7fvr22dtJJJxXX3bZtW8evLZWHNJuGzg4cOFCsN63//PPPF+v91PEprgBmBsIOJEHYgSQIO5AEYQeSIOxAEoQdSIIpm6fpqquuqq01jaN/+umnxfqFF17YUU+DcPLJJxfrd955Z7FeGkv/4IMPiuuuXLmyWF+3bl2xPmvWrNpa0zTZTb8/ee+994r1YcSeHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSYJx9mq6//vqO133mmWeK9TfffLPj1+7WyMhIsb5q1api/cQTTyzW33rrrdraFVdcUVx38+bNxXqT0jnpY2ONVz6fcdizA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EASjLNP06uvvlpbO/fcc4vrnnbaacX6scceW6w3TR9ccvXVVxfr9913X7FeOidcap6aeMmSJbW1ffv2FddFbzXu2W3Pt/287ddtb7X9k2r5HNvP2n67uj2m/+0C6NR0DuM/k/S3EfEnks6V9GPbfypplaR1EXGqpHXVYwBDqjHsETEWEZuq+x9Kel3SCZIulbSmetoaSZf1q0kA3ftan9ltL5D0fUkvSjo+Isakif8QbE95US/bI5LKP8AG0HfTDrvtb0t6QtLKiPiDPeXccV8REaOSRqvXmJETOwKHg2kNvdn+piaC/suIeLJavMf2vKo+T9J4f1oE0AuNUzZ7Yhe+RtL7EbFy0vJ/kvS/EbHa9ipJcyLi1obXOmz37KVLKjdd0niYp1xuuiTyPffcU6w3Dd1h8OqmbJ7OYfz5kq6S9JrtV6plt0taLenfbF8r6X8k/VUvGgXQH41hj4gXJNV9QP9hb9sB0C/8XBZIgrADSRB2IAnCDiRB2IEkGsfZe7qxw3icveSMM84o1u+///5iffHixb1s5xDPPfdcsX7LLbcU61u3bu1lOxiAunF29uxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kATj7MAMwzg7kBxhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJNEYdtvzbT9v+3XbW23/pFp+h+3f236l+lva/3YBdKrx4hW250maFxGbbH9H0suSLpP015L2R8Q9094YF68A+q7u4hXTmZ99TNJYdf9D269LOqG37QHot6/1md32Aknfl/RitehG25ttP2T7mJp1RmxvtL2xq04BdGXa16Cz/W1J/yHproh40vbxkvZKCkn/oIlD/b9peA0O44E+qzuMn1bYbX9T0m8k/TYi7p2ivkDSbyLizxpeh7ADfdbxBSdtW9IvJL0+OejVF3cHLZO0pdsmAfTPdL6N/4Gk/5T0mqTPq8W3S1ouaaEmDuN3SvpR9WVe6bXYswN91tVhfK8QdqD/uG48kBxhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgicYLTvbYXknvTHr83WrZMBrW3oa1L4neOtXL3k6sKwz0fPavbNzeGBGLWmugYFh7G9a+JHrr1KB64zAeSIKwA0m0HfbRlrdfMqy9DWtfEr11aiC9tfqZHcDgtL1nBzAghB1IopWw277Y9pu2t9te1UYPdWzvtP1aNQ11q/PTVXPojdveMmnZHNvP2n67up1yjr2WehuKabwL04y3+t61Pf35wD+z254l6S1JiyXtkvSSpOURsW2gjdSwvVPSooho/QcYtv9C0n5J/3xwai3b/yjp/YhYXf1HeUxE3DYkvd2hrzmNd596q5tm/Bq1+N71cvrzTrSxZz9H0vaI2BERn0r6laRLW+hj6EXEeknvf2nxpZLWVPfXaOIfy8DV9DYUImIsIjZV9z+UdHCa8Vbfu0JfA9FG2E+Q9O6kx7s0XPO9h6Tf2X7Z9kjbzUzh+IPTbFW3c1vu58sap/EepC9NMz40710n0593q42wTzU1zTCN/50fEWdLukTSj6vDVUzPzySdrIk5AMck/bTNZqppxp+QtDIi/tBmL5NN0ddA3rc2wr5L0vxJj78naXcLfUwpInZXt+OSntLEx45hsufgDLrV7XjL/XwhIvZExIGI+FzSz9Xie1dNM/6EpF9GxJPV4tbfu6n6GtT71kbYX5J0qu2TbH9L0pWSnm6hj6+wPbv64kS2Z0taouGbivppSSuq+ysk/brFXg4xLNN4100zrpbfu9anP4+Igf9JWqqJb+T/W9LftdFDTV9/LOnV6m9r271JelQTh3X/p4kjomslHStpnaS3q9s5Q9Tbv2hiau/NmgjWvJZ6+4EmPhpulvRK9be07feu0NdA3jd+LgskwS/ogCQIO5AEYQeSIOxAEoQdSIKwA0kQdiCJ/wcyJ19aO/FNTQAAAABJRU5ErkJggg==\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking the device type\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","execution_count":15,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"I am using GPU to train the model hence everything is sent to GPU via *.cuda()* command"},{"metadata":{},"cell_type":"markdown","source":"# Model Building\nOur Model is taking 1*28*28 Images as input and having the output with dimension = 10"},{"metadata":{"trusted":true},"cell_type":"code","source":"# defining the model architecture\nclass Net(nn.Module):   \n  def __init__(self):\n      super(Net, self).__init__()\n\n      self.cnn_layers = nn.Sequential(\n          # Defining a 2D convolution layer\n          nn.Conv2d(1, 4, kernel_size=3, stride=1, padding=1),\n          nn.BatchNorm2d(4),\n          nn.ReLU(inplace=True),\n          nn.MaxPool2d(kernel_size=2, stride=2),\n          # Defining another 2D convolution layer\n          nn.Conv2d(4, 4, kernel_size=3, stride=1, padding=1),\n          nn.BatchNorm2d(4),\n          nn.ReLU(inplace=True),\n          nn.MaxPool2d(kernel_size=2, stride=2),\n      )\n\n      self.linear_layers = nn.Sequential(\n          nn.Linear(4 * 7 * 7, 10)\n      )\n\n  # Defining the forward pass    \n  def forward(self, x):\n      x = self.cnn_layers(x)\n      x = x.view(x.size(0), -1)\n      x = self.linear_layers(x)\n      return x","execution_count":16,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Defining the Optimizer, Criterion (loss function) and Learning Rate Scheduler."},{"metadata":{"trusted":true},"cell_type":"code","source":"# defining the model\nmodel = Net()\n# defining the optimizer\noptimizer = optim.Adam(model.parameters(), lr=0.01)\n# defining the loss function\ncriterion = nn.CrossEntropyLoss()\nstep_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n# checking if GPU is available\nif torch.cuda.is_available():\n    model = model.cuda()\n    criterion = criterion.cuda()\n    \nprint(model)","execution_count":17,"outputs":[{"output_type":"stream","text":"Net(\n  (cnn_layers): Sequential(\n    (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (5): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (6): ReLU(inplace=True)\n    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (linear_layers): Sequential(\n    (0): Linear(in_features=196, out_features=10, bias=True)\n  )\n)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Training of the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_sizes = {}\ndataset_sizes['train'] = len(train_dataset)\ndataset_sizes['val'] = len(val_dataset)","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(model, criterion, optimizer, scheduler, num_epochs=50):\n    since = time.time() #Return the time in seconds since the epoch as a floating point number\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    dataloaders = {}\n    dataloaders['train'] = train_loader\n    dataloaders['val'] = val_loader\n    \n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            # Iterate over data.\n            for inputs, labels in dataloaders[phase]:\n                inputs = inputs.to(device)\n                inputs = inputs.type(torch.cuda.FloatTensor)\n                labels = labels.to(device)\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        optimizer.zero_grad()\n                        loss.backward()\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n\n            if phase == 'train':\n                scheduler.step()\n\n            epoch_loss = running_loss / dataset_sizes[phase]\n            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n                phase, epoch_loss, epoch_acc))\n\n            # deep copy the model\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n\n        print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed // 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodel = model.to(device)\n\nmodel = train_model(model, criterion, optimizer, step_lr_scheduler, num_epochs=50)","execution_count":20,"outputs":[{"output_type":"stream","text":"Epoch 0/49\n----------\ntrain Loss: 0.2280 Acc: 0.9289\nval Loss: 0.1374 Acc: 0.9545\n\nEpoch 1/49\n----------\ntrain Loss: 0.1135 Acc: 0.9631\nval Loss: 0.1206 Acc: 0.9631\n\nEpoch 2/49\n----------\ntrain Loss: 0.0939 Acc: 0.9704\nval Loss: 0.1113 Acc: 0.9648\n\nEpoch 3/49\n----------\ntrain Loss: 0.0865 Acc: 0.9726\nval Loss: 0.1091 Acc: 0.9679\n\nEpoch 4/49\n----------\ntrain Loss: 0.0804 Acc: 0.9746\nval Loss: 0.0980 Acc: 0.9702\n\nEpoch 5/49\n----------\ntrain Loss: 0.0765 Acc: 0.9762\nval Loss: 0.0942 Acc: 0.9707\n\nEpoch 6/49\n----------\ntrain Loss: 0.0729 Acc: 0.9768\nval Loss: 0.0980 Acc: 0.9698\n\nEpoch 7/49\n----------\ntrain Loss: 0.0514 Acc: 0.9836\nval Loss: 0.0813 Acc: 0.9755\n\nEpoch 8/49\n----------\ntrain Loss: 0.0473 Acc: 0.9851\nval Loss: 0.0822 Acc: 0.9750\n\nEpoch 9/49\n----------\ntrain Loss: 0.0463 Acc: 0.9848\nval Loss: 0.0854 Acc: 0.9745\n\nEpoch 10/49\n----------\ntrain Loss: 0.0446 Acc: 0.9856\nval Loss: 0.0851 Acc: 0.9733\n\nEpoch 11/49\n----------\ntrain Loss: 0.0445 Acc: 0.9860\nval Loss: 0.0850 Acc: 0.9748\n\nEpoch 12/49\n----------\ntrain Loss: 0.0438 Acc: 0.9861\nval Loss: 0.0869 Acc: 0.9755\n\nEpoch 13/49\n----------\ntrain Loss: 0.0428 Acc: 0.9863\nval Loss: 0.0870 Acc: 0.9726\n\nEpoch 14/49\n----------\ntrain Loss: 0.0395 Acc: 0.9877\nval Loss: 0.0862 Acc: 0.9736\n\nEpoch 15/49\n----------\ntrain Loss: 0.0389 Acc: 0.9881\nval Loss: 0.0853 Acc: 0.9736\n\nEpoch 16/49\n----------\ntrain Loss: 0.0390 Acc: 0.9881\nval Loss: 0.0857 Acc: 0.9733\n\nEpoch 17/49\n----------\ntrain Loss: 0.0387 Acc: 0.9880\nval Loss: 0.0860 Acc: 0.9731\n\nEpoch 18/49\n----------\ntrain Loss: 0.0386 Acc: 0.9882\nval Loss: 0.0861 Acc: 0.9738\n\nEpoch 19/49\n----------\ntrain Loss: 0.0386 Acc: 0.9880\nval Loss: 0.0867 Acc: 0.9731\n\nEpoch 20/49\n----------\ntrain Loss: 0.0387 Acc: 0.9882\nval Loss: 0.0863 Acc: 0.9736\n\nEpoch 21/49\n----------\ntrain Loss: 0.0382 Acc: 0.9882\nval Loss: 0.0860 Acc: 0.9738\n\nEpoch 22/49\n----------\ntrain Loss: 0.0380 Acc: 0.9883\nval Loss: 0.0863 Acc: 0.9736\n\nEpoch 23/49\n----------\ntrain Loss: 0.0380 Acc: 0.9879\nval Loss: 0.0859 Acc: 0.9738\n\nEpoch 24/49\n----------\ntrain Loss: 0.0383 Acc: 0.9882\nval Loss: 0.0862 Acc: 0.9729\n\nEpoch 25/49\n----------\ntrain Loss: 0.0381 Acc: 0.9885\nval Loss: 0.0861 Acc: 0.9736\n\nEpoch 26/49\n----------\ntrain Loss: 0.0378 Acc: 0.9885\nval Loss: 0.0861 Acc: 0.9738\n\nEpoch 27/49\n----------\ntrain Loss: 0.0380 Acc: 0.9879\nval Loss: 0.0861 Acc: 0.9733\n\nEpoch 28/49\n----------\ntrain Loss: 0.0377 Acc: 0.9882\nval Loss: 0.0862 Acc: 0.9738\n\nEpoch 29/49\n----------\ntrain Loss: 0.0379 Acc: 0.9883\nval Loss: 0.0862 Acc: 0.9738\n\nEpoch 30/49\n----------\ntrain Loss: 0.0382 Acc: 0.9884\nval Loss: 0.0862 Acc: 0.9738\n\nEpoch 31/49\n----------\ntrain Loss: 0.0383 Acc: 0.9882\nval Loss: 0.0862 Acc: 0.9736\n\nEpoch 32/49\n----------\ntrain Loss: 0.0381 Acc: 0.9881\nval Loss: 0.0868 Acc: 0.9736\n\nEpoch 33/49\n----------\ntrain Loss: 0.0380 Acc: 0.9882\nval Loss: 0.0862 Acc: 0.9738\n\nEpoch 34/49\n----------\ntrain Loss: 0.0378 Acc: 0.9884\nval Loss: 0.0863 Acc: 0.9738\n\nEpoch 35/49\n----------\ntrain Loss: 0.0381 Acc: 0.9880\nval Loss: 0.0862 Acc: 0.9738\n\nEpoch 36/49\n----------\ntrain Loss: 0.0383 Acc: 0.9880\nval Loss: 0.0862 Acc: 0.9740\n\nEpoch 37/49\n----------\ntrain Loss: 0.0379 Acc: 0.9881\nval Loss: 0.0861 Acc: 0.9740\n\nEpoch 38/49\n----------\ntrain Loss: 0.0380 Acc: 0.9887\nval Loss: 0.0862 Acc: 0.9740\n\nEpoch 39/49\n----------\ntrain Loss: 0.0374 Acc: 0.9880\nval Loss: 0.0863 Acc: 0.9738\n\nEpoch 40/49\n----------\ntrain Loss: 0.0380 Acc: 0.9878\nval Loss: 0.0862 Acc: 0.9740\n\nEpoch 41/49\n----------\ntrain Loss: 0.0382 Acc: 0.9881\nval Loss: 0.0861 Acc: 0.9740\n\nEpoch 42/49\n----------\ntrain Loss: 0.0380 Acc: 0.9879\nval Loss: 0.0861 Acc: 0.9738\n\nEpoch 43/49\n----------\ntrain Loss: 0.0382 Acc: 0.9883\nval Loss: 0.0863 Acc: 0.9745\n\nEpoch 44/49\n----------\ntrain Loss: 0.0378 Acc: 0.9883\nval Loss: 0.0866 Acc: 0.9736\n\nEpoch 45/49\n----------\ntrain Loss: 0.0381 Acc: 0.9882\nval Loss: 0.0868 Acc: 0.9736\n\nEpoch 46/49\n----------\ntrain Loss: 0.0380 Acc: 0.9884\nval Loss: 0.0861 Acc: 0.9736\n\nEpoch 47/49\n----------\ntrain Loss: 0.0384 Acc: 0.9878\nval Loss: 0.0863 Acc: 0.9740\n\nEpoch 48/49\n----------\ntrain Loss: 0.0380 Acc: 0.9882\nval Loss: 0.0862 Acc: 0.9740\n\nEpoch 49/49\n----------\ntrain Loss: 0.0380 Acc: 0.9880\nval Loss: 0.0863 Acc: 0.9738\n\nTraining complete in 1m 41s\nBest val Acc: 0.975476\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Testing our model on test-data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# getting predictions on test set and measuring the performance\ncorrect_count, all_count = 0, 0\nfor images,labels in test_loader:\n  for i in range(len(labels)):\n    images = images.cuda()\n    images = images.type(torch.cuda.FloatTensor)\n    labels = labels.cuda()\n    img = images[i].view(1, 1, 28, 28)\n    with torch.no_grad():\n        logps = model(img)\n\n    \n    ps = torch.exp(logps)\n    probab = list(ps.cpu()[0])\n    pred_label = probab.index(max(probab))\n    true_label = labels.cpu()[i]\n    if(true_label == pred_label):\n      correct_count += 1\n    all_count += 1\n\nprint(\"Number Of Images Tested =\", all_count)\nprint(\"\\nModel Accuracy =\", (correct_count/all_count))","execution_count":21,"outputs":[{"output_type":"stream","text":"Number Of Images Tested = 4200\n\nModel Accuracy = 0.9776190476190476\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Predicting for unlabelled Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"test = test_data/255 #Normalizing the data\ntest = torch.from_numpy(test)  # Converting into Tensors\ntest = test.type(torch.cuda.FloatTensor) ","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with torch.no_grad():\n  outputs = model(test.cuda())","execution_count":23,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Outputs are the output of the final linear Layer having shape = 10"},{"metadata":{"trusted":true},"cell_type":"code","source":"ps = torch.exp(outputs)\n\n#max_value is the value of highest no. in each 10-dim vector \n#index is the index of that max value \nmax_value, index = torch.max(ps,axis=1) \n\nindex = index.cpu()\n#Converting Prediction to numpy for Submission\nprediction = index.numpy()\n\nprint(prediction.shape)\nprint(prediction[:5])","execution_count":24,"outputs":[{"output_type":"stream","text":"(28000,)\n[2 0 9 0 3]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Saving the Prediction in the acceptable format "},{"metadata":{"trusted":true},"cell_type":"code","source":"k = np.arange(1,28001)\nsubmission = pd.DataFrame({\n        \"ImageId\":k ,\n        \"Label\": prediction\n\n    })\n\nsubmission.to_csv('Digit_Recognition_submission.csv', index=False)","execution_count":25,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}